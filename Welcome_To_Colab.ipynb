{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenamilian/AllstateML/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/tenx-train-baselines.zip -d .\n",
        "%cd tenx-train"
      ],
      "metadata": {
        "id": "Abwc2iK44cLy",
        "outputId": "1add1997-bda9-45ba-890d-f9637e20910f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace ./tenx-train/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: /content/tenx-train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tenx-train\n",
        "!ls"
      ],
      "metadata": {
        "id": "rtJ-GA1i7nVb",
        "outputId": "0259a3a4-cd71-4a88-b774-65833baa8c03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tenx-train\n",
            "configs        models.py    reports\t      train_baseline.py\n",
            "data_utils.py  __pycache__  requirements.txt  utils.py\n",
            "emissions.csv  README.md    tenx-train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --no-deps -q\n"
      ],
      "metadata": {
        "id": "M4w8t1fb6krQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "import torch, gc\n",
        "gc.collect(); torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3VGH_khlF2xx",
        "outputId": "b37d191a-1265-4953-d1c1-86b1bd231fe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"data_utils.py\",\"w\").write(\"\"\"\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2TokenizerFast\n",
        "import torch\n",
        "\n",
        "def load_tokenizer(name=\"gpt2\", seq_len=1024):\n",
        "    tok = GPT2TokenizerFast.from_pretrained(name)\n",
        "    tok.model_max_length = seq_len\n",
        "    return tok\n",
        "\n",
        "def load_and_tokenize(dataset: str, split: str, tok, num_proc=2, cfg_name: str=None):\n",
        "    if cfg_name:\n",
        "        ds = load_dataset(dataset, cfg_name, split=split)\n",
        "    else:\n",
        "        ds = load_dataset(dataset, split=split)\n",
        "    def enc(e): return {\"ids\": tok(e[\"text\"], add_special_tokens=False)[\"input_ids\"]}\n",
        "    return ds.map(enc, remove_columns=ds.column_names, num_proc=num_proc)\n",
        "\n",
        "def pack_sequences(token_ds, seq_len=1024):\n",
        "    buf, out = [], []\n",
        "    for ex in token_ds:\n",
        "        buf.extend(ex[\"ids\"])\n",
        "        while len(buf) >= seq_len:\n",
        "            out.append(buf[:seq_len]); buf = buf[seq_len:]\n",
        "    return out\n",
        "\n",
        "class PackedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, seqs): self.seqs=seqs\n",
        "    def __len__(self): return len(self.seqs)\n",
        "    def __getitem__(self, i): return torch.tensor(self.seqs[i], dtype=torch.long)\n",
        "\"\"\")\n",
        "print(\"✅ Patched data_utils.py\")"
      ],
      "metadata": {
        "id": "-IUAtMN1BON6",
        "outputId": "41ded90a-d591-4dfb-a8aa-25f3f6bdad58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "code = open(\"train_baseline.py\").read()\n",
        "\n",
        "code = re.sub(\n",
        "    r'ds_train\\s*=\\s*load_and_tokenize\\(.*\\)',\n",
        "    'ds_train = load_and_tokenize(data_cfg[\"dataset\"], split=data_cfg.get(\"train_split\",\"train\"), tok=tok, cfg_name=data_cfg.get(\"dataset_config\"))',\n",
        "    code, count=1\n",
        ")\n",
        "code = re.sub(\n",
        "    r'ds_val\\s*=\\s*load_and_tokenize\\(.*\\)',\n",
        "    'ds_val   = load_and_tokenize(data_cfg[\"dataset\"], split=data_cfg.get(\"val_split\",\"validation\"), tok=tok, cfg_name=data_cfg.get(\"dataset_config\"))',\n",
        "    code, count=1\n",
        ")\n",
        "\n",
        "open(\"train_baseline.py\",\"w\").write(code)\n",
        "print(\"✅ Patched train_baseline.py\")"
      ],
      "metadata": {
        "id": "-R77cKH2BPM9",
        "outputId": "523aec51-d27a-4270-8c0a-139d67543207",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched train_baseline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "path = \"models.py\"\n",
        "code = open(path).read()\n",
        "\n",
        "# Replace the whole SDPA_Attn class definition with the fixed version\n",
        "patched = re.sub(\n",
        "    r\"class SDPA_Attn\\(nn\\.Module\\):[\\s\\S]*?class\",   # match until the next class\n",
        "    \"\"\"\n",
        "class SDPA_Attn(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, rope=False):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.rope = rope\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        if self.rope:\n",
        "            q, k = apply_rope(q, k)\n",
        "\n",
        "        # (B,H,T,D) → (B,T,H,D)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # causal attention without explicit mask\n",
        "        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        # (B,T,H,D) → (B,T,C)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.proj(out)\n",
        "\n",
        "class\"\"\",\n",
        "    code,\n",
        "    count=1,\n",
        "    flags=re.MULTILINE\n",
        ")\n",
        "\n",
        "open(path, \"w\").write(patched)\n",
        "print(\"✅ Patched models.py to use causal SDPA without attn_mask\")"
      ],
      "metadata": {
        "id": "aloLvPvFCQ9E",
        "outputId": "8f6b4d39-ff20-4ec4-810a-d3d2807ed746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched models.py to use causal SDPA without attn_mask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "yaml_text = \"\"\"model:\n",
        "  variant: modern\n",
        "  n_layers: 4\n",
        "  d_model: 320\n",
        "  n_heads: 5\n",
        "  vocab_size: 50257\n",
        "  seq_len: 256\n",
        "\n",
        "train:\n",
        "  precision: fp16\n",
        "  optimizer: adamw\n",
        "  lr: 3.0e-4\n",
        "  betas: [0.9, 0.95]\n",
        "  weight_decay: 0.1\n",
        "  warmup_steps: 200\n",
        "  global_batch_tokens: 16384   # e.g., 64×256\n",
        "  max_tokens: 800_000\n",
        "\n",
        "data:\n",
        "  dataset: wikitext\n",
        "  dataset_config: wikitext-2-raw-v1\n",
        "  train_split: train\n",
        "  val_split: validation\n",
        "  tokenizer: gpt2\n",
        "\n",
        "logging:\n",
        "  log_interval: 50\n",
        "  eval_interval: 400\n",
        "\n",
        "system:\n",
        "  seed: 1337\n",
        "\"\"\"\n",
        "Path(\"configs/gold_colab_free.yaml\").write_text(yaml_text)\n",
        "print(\"✅ wrote smaller gold_colab_free.yaml\")"
      ],
      "metadata": {
        "id": "kacar1lTF-Ke",
        "outputId": "075bf2e2-3e44-4181-9689-8208eb6b9f39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ wrote smaller gold_colab_free.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "path = \"models.py\"\n",
        "code = open(path).read()\n",
        "\n",
        "# add checkpoint import\n",
        "if \"from torch.utils.checkpoint import checkpoint\" not in code:\n",
        "    code = code.replace(\n",
        "        \"import torch, torch.nn as nn, torch.nn.functional as F\",\n",
        "        \"import torch, torch.nn as nn, torch.nn.functional as F\\nfrom torch.utils.checkpoint import checkpoint\"\n",
        "    )\n",
        "\n",
        "# wrap block calls with checkpoint\n",
        "code = re.sub(r\"for b in self.blocks:\\n\\s*x = b\\(x\\)\",\n",
        "              \"for b in self.blocks:\\n            x = checkpoint(b, x)\",\n",
        "              code)\n",
        "\n",
        "open(path,\"w\").write(code)\n",
        "print(\"✅ Enabled gradient checkpointing in models.py\")"
      ],
      "metadata": {
        "id": "AI6X-NC2GCVH",
        "outputId": "ecbe66bc-b7be-439a-d12e-a53c899af71b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Enabled gradient checkpointing in models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_baseline.py --config configs/gold_colab_free.yaml"
      ],
      "metadata": {
        "id": "LJ3cvrG5BZKN",
        "outputId": "464e357a-eea6-46a2-a0a6-963f48f07e84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "Loading & tokenizing...\n",
            "Map (num_proc=2):   0% 0/36718 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (324 > 256). Running this sequence through the model will result in indexing errors\n",
            "Map (num_proc=2):   0% 84/36718 [00:00<02:27, 247.70 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (269 > 256). Running this sequence through the model will result in indexing errors\n",
            "Map (num_proc=2): 100% 36718/36718 [00:10<00:00, 3553.88 examples/s]\n",
            "Map (num_proc=2):   0% 0/3760 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (318 > 256). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (297 > 256). Running this sequence through the model will result in indexing errors\n",
            "Map (num_proc=2): 100% 3760/3760 [00:01<00:00, 2813.86 examples/s]\n",
            "Packing sequences...\n",
            "Params: 37,081,280 | Seq len: 256 | Batch (seqs): 64\n",
            "Optimizer: AdamW8bit (bitsandbytes)\n",
            "/content/tenx-train/train_baseline.py:57: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=use_amp)\n",
            "[codecarbon WARNING @ 18:59:11] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 18:59:11] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 18:59:11] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 18:59:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 18:59:12] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 18:59:12] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 18:59:12] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 18:59:12] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 18:59:12] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 18:59:12] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 18:59:12] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 18:59:12]   Platform system: Linux-6.1.123+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 18:59:12]   Python version: 3.12.11\n",
            "[codecarbon INFO @ 18:59:12]   CodeCarbon version: 3.0.4\n",
            "[codecarbon INFO @ 18:59:12]   Available RAM : 12.674 GB\n",
            "[codecarbon INFO @ 18:59:12]   CPU count: 2 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 18:59:12]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 18:59:12]   GPU count: 1\n",
            "[codecarbon INFO @ 18:59:12]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 18:59:12] Emissions data (if any) will be saved to file /content/tenx-train/emissions.csv\n",
            "/content/tenx-train/train_baseline.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp, dtype=amp_dtype):\n",
            "[codecarbon INFO @ 18:59:27] Energy consumed for RAM : 0.000042 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:59:27] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:59:27] Energy consumed for All CPU : 0.000177 kWh\n",
            "[codecarbon INFO @ 18:59:27] Energy consumed for all GPUs : 0.000280 kWh. Total GPU Power : 67.26145895457651 W\n",
            "[codecarbon INFO @ 18:59:27] 0.000499 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 18:59:32] Energy consumed for RAM : 0.000054 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 18:59:32] Delta energy consumed for CPU with constant : 0.000050 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 18:59:32] Energy consumed for All CPU : 0.000228 kWh\n",
            "[codecarbon INFO @ 18:59:32] Energy consumed for all GPUs : 0.000363 kWh. Total GPU Power : 69.95159747311831 W\n",
            "[codecarbon INFO @ 18:59:32] 0.000645 kWh of electricity used since the beginning.\n",
            "=== SUMMARY ===\n",
            "variant: modern\n",
            "params: 37081280\n",
            "tokens_seen: 802816\n",
            "approx_flops: 178616669306880.0\n",
            "kwh: 8.943331253243888e-05\n",
            "best_val_loss: inf\n",
            "runtime_sec: 20.592370748519897\n",
            "seq_len: 256\n",
            "batch_size: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "plNF_-PNIhaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate(N_params, T_tokens, eff_tflops=150, gpus=256, power_w=300):\n",
        "    flops = 6 * N_params * T_tokens\n",
        "    secs = flops / (eff_tflops * 1e12 * gpus)\n",
        "    kwh = (power_w * gpus) * secs / 3600 / 1000\n",
        "    return dict(FLOPs=flops, days=secs/86400, kWh=kwh)\n",
        "\n",
        "# examples\n",
        "print(estimate(7e9, 20*7e9, eff_tflops=150, gpus=256, power_w=300))\n",
        "print(estimate(70e9, 20*70e9, eff_tflops=150, gpus=1024, power_w=350))"
      ],
      "metadata": {
        "id": "tB6lV_G0IYad",
        "outputId": "b0046e39-396d-4fa3-95df-582d3907fbb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'FLOPs': 5.88e+21, 'days': 1.7722800925925926, 'kWh': 3266.6666666666665}\n",
            "{'FLOPs': 5.88e+23, 'days': 44.30700231481482, 'kWh': 381111.1111111111}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) CUDA allocator: reduce fragmentation\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
        "import torch, gc, re\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# 1) Enable gradient checkpointing in both models (big memory saver)\n",
        "path = \"models.py\"\n",
        "code = open(path).read()\n",
        "if \"from torch.utils.checkpoint import checkpoint\" not in code:\n",
        "    code = code.replace(\n",
        "        \"import torch, torch.nn as nn, torch.nn.functional as F\",\n",
        "        \"import torch, torch.nn as nn, torch.nn.functional as F\\nfrom torch.utils.checkpoint import checkpoint\"\n",
        "    )\n",
        "code = re.sub(r\"for b in self.blocks:\\n\\s*x = b\\(x\\)\",\n",
        "              \"for b in self.blocks:\\n            x = checkpoint(b, x)\",\n",
        "              code)\n",
        "open(path, \"w\").write(code)\n",
        "print(\"✅ Enabled gradient checkpointing\")\n",
        "\n",
        "# 2) Make DataLoaders lighter (avoid extra RAM/VRAM pressure)\n",
        "p = \"train_baseline.py\"\n",
        "s = open(p).read()\n",
        "s = s.replace(\n",
        "    \"torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\",\n",
        "    \"torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0, pin_memory=False)\"\n",
        ")\n",
        "s = s.replace(\n",
        "    \"torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\",\n",
        "    \"torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=0, pin_memory=False)\"\n",
        ")\n",
        "open(p, \"w\").write(s)\n",
        "print(\"✅ Patched DataLoaders\")\n",
        "\n",
        "# 3) Write tiny Colab-safe configs for both baselines\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "gold_tiny = \"\"\"model:\n",
        "  variant: modern              # RMSNorm + RoPE + SwiGLU + SDPA\n",
        "  n_layers: 4\n",
        "  d_model: 320\n",
        "  n_heads: 5\n",
        "  vocab_size: 50257\n",
        "  seq_len: 256                 # shorter context to cut memory\n",
        "\n",
        "train:\n",
        "  precision: fp16              # T4 is happiest with fp16\n",
        "  optimizer: adamw             # simpler; a tad less VRAM than bnb\n",
        "  lr: 3.0e-4\n",
        "  betas: [0.9, 0.95]\n",
        "  weight_decay: 0.1\n",
        "  warmup_steps: 200\n",
        "  global_batch_tokens: 12288   # e.g., 48×256; lower if OOM: 8192 (32×256)\n",
        "  max_tokens: 600_000\n",
        "\n",
        "data:\n",
        "  dataset: wikitext\n",
        "  dataset_config: wikitext-2-raw-v1\n",
        "  train_split: train\n",
        "  val_split: validation\n",
        "  tokenizer: gpt2\n",
        "\n",
        "logging:\n",
        "  log_interval: 50\n",
        "  eval_interval: 400\n",
        "\n",
        "system:\n",
        "  seed: 1337\n",
        "\"\"\"\n",
        "Path(\"configs/gold_colab_free.yaml\").write_text(gold_tiny)\n",
        "\n",
        "historic_tiny = \"\"\"model:\n",
        "  variant: historic            # LayerNorm + GELU + absolute pos\n",
        "  n_layers: 4\n",
        "  d_model: 320\n",
        "  n_heads: 5\n",
        "  vocab_size: 50257\n",
        "  seq_len: 256\n",
        "\n",
        "train:\n",
        "  precision: fp16\n",
        "  optimizer: adamw\n",
        "  lr: 3.0e-4\n",
        "  betas: [0.9, 0.95]\n",
        "  weight_decay: 0.1\n",
        "  warmup_steps: 200\n",
        "  global_batch_tokens: 12288   # match gold for fair compare\n",
        "  max_tokens: 600_000\n",
        "\n",
        "data:\n",
        "  dataset: wikitext\n",
        "  dataset_config: wikitext-2-raw-v1\n",
        "  train_split: train\n",
        "  val_split: validation\n",
        "  tokenizer: gpt2\n",
        "\n",
        "logging:\n",
        "  log_interval: 50\n",
        "  eval_interval: 400\n",
        "\n",
        "system:\n",
        "  seed: 1337\n",
        "\"\"\"\n",
        "Path(\"configs/historic_colab_free.yaml\").write_text(historic_tiny)\n",
        "print(\"✅ Wrote tiny configs for gold & historic\")"
      ],
      "metadata": {
        "id": "HQ-1ZJfDHDMg",
        "outputId": "d03cb3be-ead9-4696-cbcc-bb474098a355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True\n",
            "✅ Enabled gradient checkpointing\n",
            "✅ Patched DataLoaders\n",
            "✅ Wrote tiny configs for gold & historic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_baseline.py --config configs/historic_colab_free.yaml"
      ],
      "metadata": {
        "id": "UvaRjAmSBcff",
        "outputId": "b3e07b1e-9174-4e31-9013-928aff4badf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "Loading & tokenizing...\n",
            "Packing sequences...\n",
            "Params: 37,167,360 | Seq len: 256 | Batch (seqs): 48\n",
            "Optimizer: AdamW8bit (bitsandbytes)\n",
            "/content/tenx-train/train_baseline.py:57: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=use_amp)\n",
            "[codecarbon WARNING @ 19:03:11] Multiple instances of codecarbon are allowed to run at the same time.\n",
            "[codecarbon INFO @ 19:03:11] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 19:03:11] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 19:03:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.20GHz but we don't know it. Please contact us.\n",
            "[codecarbon WARNING @ 19:03:12] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
            " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
            "\n",
            "[codecarbon INFO @ 19:03:12] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon WARNING @ 19:03:12] No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "[codecarbon INFO @ 19:03:12] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 19:03:12] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 19:03:12] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: RAM power estimation model\n",
            "                CPU Tracking Method: global constant\n",
            "                GPU Tracking Method: pynvml\n",
            "            \n",
            "[codecarbon INFO @ 19:03:12] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 19:03:12]   Platform system: Linux-6.1.123+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 19:03:12]   Python version: 3.12.11\n",
            "[codecarbon INFO @ 19:03:12]   CodeCarbon version: 3.0.4\n",
            "[codecarbon INFO @ 19:03:12]   Available RAM : 12.674 GB\n",
            "[codecarbon INFO @ 19:03:12]   CPU count: 2 thread(s) in 1 physical CPU(s)\n",
            "[codecarbon INFO @ 19:03:12]   CPU model: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "[codecarbon INFO @ 19:03:12]   GPU count: 1\n",
            "[codecarbon INFO @ 19:03:12]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 19:03:12] Emissions data (if any) will be saved to file /content/tenx-train/emissions.csv\n",
            "/content/tenx-train/train_baseline.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp, dtype=amp_dtype):\n",
            "[codecarbon INFO @ 19:03:27] Energy consumed for RAM : 0.000042 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 19:03:27] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 19:03:27] Energy consumed for All CPU : 0.000177 kWh\n",
            "[codecarbon INFO @ 19:03:27] Energy consumed for all GPUs : 0.000252 kWh. Total GPU Power : 60.39020705717562 W\n",
            "[codecarbon INFO @ 19:03:27] 0.000470 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 19:03:28] Energy consumed for RAM : 0.000044 kWh. RAM Power : 10.0 W\n",
            "[codecarbon INFO @ 19:03:28] Delta energy consumed for CPU with constant : 0.000010 kWh, power : 42.5 W\n",
            "[codecarbon INFO @ 19:03:28] Energy consumed for All CPU : 0.000187 kWh\n",
            "[codecarbon INFO @ 19:03:28] Energy consumed for all GPUs : 0.000267 kWh. Total GPU Power : 66.93830383044467 W\n",
            "[codecarbon INFO @ 19:03:28] 0.000497 kWh of electricity used since the beginning.\n",
            "=== SUMMARY ===\n",
            "variant: historic\n",
            "params: 37167360\n",
            "tokens_seen: 602112\n",
            "approx_flops: 134273480785920.0\n",
            "kwh: 6.902510971911876e-05\n",
            "best_val_loss: inf\n",
            "runtime_sec: 17.197157859802246\n",
            "seq_len: 256\n",
            "batch_size: 48\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}